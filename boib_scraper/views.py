# boib_scraper/views.py

import requests
import datetime
import warnings
import pandas as pd
from bs4 import BeautifulSoup
from bs4 import XMLParsedAsHTMLWarning
from django.db import models
from django.shortcuts import render
from django.utils.timezone import now, timedelta
from .models import AnuncioBoib

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

def current_bulletin_id():
  """Escrapea los datos en el apartado de 칰ltima publicaci칩n de bolet칤n
  y devuelve la 'url_id del 칰ltimo ejemplar publicado"""
  last_bulletin_url = 'http://www.caib.es/eboibfront/'
  page = requests.get(last_bulletin_url)
  soup = BeautifulSoup(page.content, 'lxml')
  # Devuelve el n췈 de url del ultimo boletin disponible
  links = soup.find_all("a", href=lambda href: href and "/eboibfront/ca/" in href)
  link_list = [link["href"].split('/')[-2] for link in links]
  return link_list[0]

class BulletinScraper:
    """Clase con los atributos y funciones comunes para
    realizar scraping sobre cualquier Bolet칤n Oficial"""

    def __init__(self, url_id):
        self.url_id = int(url_id)
        self.date_list = []
        self.bulletin_list = []
        self.number_list = []
        self.authority_list = []
        self.entity_list = []
        self.resolution_list = []
        self.register_list = []
        self.pdf_link_list = []
        self.html_link_list = []
        self.xml_link_list = []
        self.text_list = []
        self.url_number_list = []
        self.data = pd.DataFrame()
        self.json_list = []

    def clear_lists(self):
        """Vac칤a los atributos de la clase BulletinScraper"""
        self.date_list = []
        self.bulletin_list = []
        self.number_list = []
        self.authority_list = []
        self.entity_list = []
        self.resolution_list = []
        self.register_list = []
        self.pdf_link_list = []
        self.html_link_list = []
        self.xml_link_list = []
        self.text_list = []
        self.url_number_list = []
        self.data = pd.DataFrame()
        self.json_list = []

    def make_dataframe(self):
        """Crea un dataframe con cabeceras desde los datos guardados
        en las listas de la clase BulletinScraper"""
        list_merge = {'URL_id': self.url_id,
                      'Fecha': self.date_list,
                      'Bolet칤n': self.bulletin_list,
                      'N칰mero': self.number_list,
                      'Administraci칩n': self.authority_list,
                      'Entidad': self.entity_list,
                      'Resoluci칩n': self.resolution_list,
                      'PDF': self.pdf_link_list,
                      'HTML': self.html_link_list,
                      'XML': self.xml_link_list,
                      'Texto': self.text_list,
                      'N췈 Registro': self.register_list,
                      'N췈 URL': self.url_number_list}
        self.data = pd.DataFrame(data=list_merge)
        #pd.set_option('display.max_columns', None)
        return self.data

    def make_json(self):
        """Crea un json con cabeceras desde los datos guardados
        en las listas de la clase BulletinScraper"""

        for element in range(len(self.date_list)):
            merged_ele = {'URL_id': self.url_id,
                          'Fecha': self.date_list[element].isoformat(),
                          'Boletin': self.bulletin_list[element],
                          'Numero': self.number_list[element],
                          'Administracion': self.authority_list[element],
                          'Entidad': self.entity_list[element],
                          'Resolucion': self.resolution_list[element],
                          'PDF': self.pdf_link_list[element],
                          'HTML': self.html_link_list[element],
                          'XML': self.xml_link_list[element],
                          'Texto': self.text_list[element],
                          'N_Registro': self.register_list[element],
                          'N_URL': self.url_number_list[element]}
            self.json_list.append(merged_ele)


class BoibScraper(BulletinScraper):
    """ Clase con los atributos y funciones espec칤ficas para
    realizar scraping sobre el Boletin Oficial de les Illes Balears.
    Se le debe pasar el par치metro 'url_id' con formato 'numero'"""
    def __init__(self, url_id='10140'): #el mas antiguo es '10140'
        super().__init__(url_id)
        self.url = 'https://www.caib.es/eboibfront/es/2020/' + str(url_id) + '/seccio-ii-autoritats-i-personal/473'
        self.current_bulletin_id = self.current_bulletin_id()

    def announcement_scraping_loop(self, block_2_2):
        """ Rasca los datos de la web y los guarda en los atributos de la clase BulletinScraper
        :param block_2_2: Ubicaci칩n html de la secci칩n de anuncios de oposiciones y concursos
        :return: None
        """
        announcements = block_2_2.find_all('div', {'class': 'caja'})
        for announcement in announcements:
            links_bs4 = announcement.findAll('a', href=True)
            for link in links_bs4:

                if link['class'] == ['pdf']:
                    full_link = "https://www.caib.es" + link['href']
                    self.pdf_link_list.append(full_link)

                elif link['class'] == ['html']:
                    full_link = link['href']
                    # Link HMTL
                    self.html_link_list.append(full_link)
                    page = requests.get(full_link)
                    soup = BeautifulSoup(page.content, 'lxml')
                    text = str(soup.find('div', {'id': 'contenidoEdicto'}).get_text())
                    text_cleaned = ' '.join(text.split())[:10000]
                    # Texto anuncio (limitado a 10000 car치cteres)
                    self.text_list.append(text_cleaned)

                elif link['class'] == ['rdf']:
                    full_link = link['href']
                    page = requests.get(full_link)
                    soup = BeautifulSoup(page.content, 'lxml')

                    # Link XML
                    self.xml_link_list.append(full_link)

                    # Fecha
                    publication_date = str(soup.find_all('dc:date')[0]).replace('<dc:date>', '').replace('</dc:date>',
                                                                                                         '')
                    publication_date = datetime.datetime.strptime(publication_date, '%Y-%m-%d').date()
                    self.date_list.append(publication_date)
                    # N췈Registro
                    self.register_list.append(
                        str(soup.find_all('env:numeroregistre')).replace('[<env:numeroregistre>', '').replace(
                            '</env:numeroregistre>]', ''))
                    # N췈URL
                    self.url_number_list.append(str(full_link).split('/')[-3])
                    # print(str(full_link).split('/')[-3])

                else:
                    print('Error en el scraping de links')
                    break

            # Nombre de bolet칤n
            self.bulletin_list.append("BOIB")
            # N칰mero del bolet칤n
            page = requests.get(self.url)
            soup = BeautifulSoup(page.content, 'lxml')
            num = soup.find_all('strong')
            number = int(num[1].get_text().replace('N칰m.', ''))
            self.number_list.append(str(number))
            # Nombre de entidad
            entity_bs4 = BeautifulSoup(announcement.find('h3', {'class': 'organisme'}).find('strong').text, 'lxml')
            entity_text = entity_bs4.get_text()
            self.entity_list.append(entity_text)
            # Nombre de la Administraci칩n
            authority_bs4 = BeautifulSoup(announcement.find('h3', {'class': 'organisme'}).text, 'lxml')
            authority_text = authority_bs4.get_text().strip('\n')
            if authority_text != entity_text:
                authority_text = authority_text.replace(entity_text, '')
            self.authority_list.append(authority_text)
            # Resoluci칩n completa
            resolution_bs4 = BeautifulSoup(announcement.find('ul', {'class': 'resolucions'}).text, 'lxml')
            resolution_text = resolution_bs4.get_text().replace('\r', ' ').replace('\n', '')
            resolution_text = resolution_text.replace('\t', '').split('N칰mero')
            self.resolution_list.append(resolution_text[0])

    def get_data(self):
        """ Comprueba que existan los apartados del BOIB y ejecuta el loop sobre los anuncios.
        Guarda el n칰mero de BOIB con errores, si los hay"""
        page = requests.get(self.url)
        soup = BeautifulSoup(page.content, 'lxml')
        block_2 = soup.find_all('ul', {'class': 'entitats'})
        tester = soup.find_all('ul', {'class': 'llistat'})
        
        # Comprobamos que existe la secci칩n primera del apartado de nombramientos y autoridades:
        try:
            # Si existe la secci칩n primera, instanciamos la secci칩n segunda
            if str(tester[0]).find('Subsecci칩n primera.') >= 0:
                block_2_2 = block_2[1]
                # Recorremos los anuncios de la secci칩n segunda
                self.announcement_scraping_loop(block_2_2)

            # Si no existe la secci칩n primera:
            else:
                # Si existe la secci칩n segunda, la instanciamos:
                if str(tester[0]).find('Subsecci칩n segunda.') >= 0:
                    block_2_2 = block_2[0]
                    # Recorremos los anuncios de la secci칩n segunda
                    self.announcement_scraping_loop(block_2_2)

                # Si no existe la secci칩n primera ni segunda:
                else:
                    print('No hay oposiciones/convocatorias en el BOIB con URL: ', self.url)
            df = self.make_dataframe()
            #self.make_dataframe()
            self.make_json()
            return df 
        except:
            print('No existe el BOIB con URL: ', self.url)

    def current_bulletin_id(self):
        """Escrapea los datos en el apartado de 칰ltima publicaci칩n de bolet칤n
        y devuelve la 'url_id del 칰ltimo ejemplar publicado"""
        last_bulletin_url = 'http://www.caib.es/eboibfront/'
        page = requests.get(last_bulletin_url)
        soup = BeautifulSoup(page.content, 'lxml')
        # Devuelve el n췈 de url del ultimo boletin disponible
        links = soup.find_all("a", href=lambda href: href and "/eboibfront/ca/" in href)
        link_list = [link["href"].split('/')[-2] for link in links]
        return link_list[0]
    
    def scrape_boib(self):
        print(f'游댍 Intentando rascar datos de {self.url_id}')
        # Aqu칤 pones la l칩gica de scraping
        print(f'游댌 Datos obtenidos: {self.url}')
    
def dataframe_to_db(df):
    """
    Guarda un DataFrame en la base de datos sin duplicar registros existentes.
    Usa 'numero_url' como identificador 칰nico.
    """
    # Obtener los valores 칰nicos de 'numero_url' ya almacenados en la BD
    urls_existentes = set(AnuncioBoib.objects.values_list("numero_url", flat=True))
    
    if df is None or df.empty:
        print(f'el BOIB no existe')
    
    else:
        nuevos_anuncios = [
            AnuncioBoib(
                url_id=row["URL_id"],
                fecha=pd.to_datetime(row["Fecha"]).date(),
                boletin=row["Bolet칤n"],
                numero_boletin=row["N칰mero"],
                administracion=row["Administraci칩n"],
                entidad=row["Entidad"],
                texto_resolucion=row["Resoluci칩n"],
                link_pdf=row["PDF"],
                link_html=row["HTML"],
                link_xml=row["XML"],
                texto_completo=row["Texto"],
                numero_registro=row["N췈 Registro"],
                numero_url=row["N췈 URL"],  # Clave 칰nica para evitar duplicados
            )
            for _, row in df.iterrows() if row["N췈 URL"] not in urls_existentes
        ]

        if nuevos_anuncios:
            AnuncioBoib.objects.bulk_create(nuevos_anuncios)  # Inserta solo los nuevos

        return f"Se han insertado {len(nuevos_anuncios)} nuevos registros."

def get_max_url_id():
    """
    Devuelve el valor m치ximo de 'url_id' en la base de datos (el 칰ltimo bolet칤n guardado en db).
    Si la tabla est치 vac칤a, retorna 12036.
    """
    max_url_id = AnuncioBoib.objects.aggregate(max_id=models.Max("url_id"))["max_id"]
    return max_url_id if max_url_id is not None else '12035'

def update_db():
    current_id = int(current_bulletin_id())
    max_url_id = int(get_max_url_id()) 
    print('El ultimo BOIB publicado es:', current_id)
    print('El ultimo BOIB guardado en DB es:', max_url_id)
    if current_id > max_url_id:
        while current_id > max_url_id:
            print(f'Actualizando BOIB: {str(max_url_id + 1)}')
            boib = BoibScraper(str(max_url_id + 1))
            df = boib.get_data()
            if df is None: 
                print('BOIB inexistente')
            else:
                dataframe_to_db(df)
                print(f'BOIB {max_url_id + 1} guardado con 칠xito')
            max_url_id += 1
        return f"BD actualizada! ID del 칰ltimo BOIB: {str(max_url_id + 1)}"

def anuncios_recientes(request):
    """Actualiza la base de datos y la vuelca en una tabla que se renderiza en
    'boib_scraper/anuncios.html'
    """
    #Actualiza la base de datos
    update_db()
    #boib = BoibScraper().get_data()
    #print(boib)
    # Obtener la fecha de hace 20 d칤as
    fecha_limite = now().date() - timedelta(days=30)
    # Filtrar los anuncios de los 칰ltimos 20 d칤as y ordenarlos por fecha descendente
    anuncios = AnuncioBoib.objects.filter(fecha__gte=fecha_limite).order_by('-fecha')
    return render(request, 'boib_scraper/anuncios.html', {'anuncios': anuncios})